{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea93e91d-4363-4fc4-8777-a3d34ee41013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 187 images belonging to 6 classes.\n",
      "Found 18 images belonging to 6 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 10s/step - accuracy: 0.2154 - loss: 2.2277 - val_accuracy: 0.2778 - val_loss: 1.6025\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 11s/step - accuracy: 0.2629 - loss: 1.8009 - val_accuracy: 0.4444 - val_loss: 1.4524\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 10s/step - accuracy: 0.4578 - loss: 1.4303 - val_accuracy: 0.3889 - val_loss: 1.3317\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10s/step - accuracy: 0.5461 - loss: 1.2056 - val_accuracy: 0.6111 - val_loss: 1.1715\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 10s/step - accuracy: 0.6281 - loss: 0.9885 - val_accuracy: 0.5556 - val_loss: 1.1726\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 10s/step - accuracy: 0.6414 - loss: 0.8927 - val_accuracy: 0.6667 - val_loss: 1.0435\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 11s/step - accuracy: 0.7073 - loss: 0.8535 - val_accuracy: 0.5556 - val_loss: 1.0848\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 10s/step - accuracy: 0.8054 - loss: 0.6840 - val_accuracy: 0.7222 - val_loss: 0.9122\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 9s/step - accuracy: 0.8066 - loss: 0.5948 - val_accuracy: 0.5556 - val_loss: 0.8684\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 12s/step - accuracy: 0.8463 - loss: 0.5527 - val_accuracy: 0.7222 - val_loss: 0.8670\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define relevant classes (e.g., for animals that interact with crops/livestock)\n",
    "animal_classes = [\n",
    "    'n01560419',  # Crow\n",
    "    'n01817953',  # Pigeon\n",
    "    'n01558993',  # European Starling\n",
    "    'n01532829',  # Sparrow\n",
    "    'n01843383',  # Hornbill\n",
    "    'n01855672'   # Bee-eater\n",
    "]\n",
    "\n",
    "# Directory paths to your ImageNet dataset\n",
    "train_dir = 'data/train'\n",
    "val_dir = 'data/val'\n",
    "\n",
    "\n",
    "# Define ImageDataGenerators\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    classes=[str(cls) for cls in animal_classes]\n",
    ")\n",
    "\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),  # ImageNet standard size\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    classes=[str(cls) for cls in animal_classes]\n",
    ")\n",
    "\n",
    "\n",
    "# Available classes\n",
    "class_labels = list(train_data.class_indices.keys())\n",
    "\n",
    "# Load the pre-trained model (VGG16) without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(animal_classes), activation='softmax')(x)  # Update to the number of classes\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, epochs=10, validation_data=val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1188a587-7065-4898-b193-8ceba9d2cac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step\n",
      "Predicted Class Index: n01532829, Probability: 0.40898844599723816\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "def predict_image_class(model, img_path, class_labels):\n",
    "    # Load and preprocess the image\n",
    "    img_array = load_and_preprocess_image(img_path)\n",
    "    \n",
    "    # Predict the class\n",
    "    predictions = model.predict(img_array)\n",
    "    class_index = np.argmax(predictions[0])\n",
    "    predicted_label = class_labels[class_index]\n",
    "    \n",
    "    return predicted_label, predictions[0][class_index]\n",
    "\n",
    "# Example usage\n",
    "img_path = 'jack.png'\n",
    "class_index, probability = predict_image_class(model, img_path, class_labels)\n",
    "print(f\"Predicted Class Index: {class_index}, Probability: {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "796709d5-2f93-495f-a752-2b5dd008fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5s/step - accuracy: 0.1494 - loss: 3.8924 - val_accuracy: 0.1667 - val_loss: 2.7996\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 5s/step - accuracy: 0.1742 - loss: 3.3511 - val_accuracy: 0.1667 - val_loss: 2.0430\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 5s/step - accuracy: 0.1743 - loss: 2.5463 - val_accuracy: 0.2778 - val_loss: 2.1494\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5s/step - accuracy: 0.2619 - loss: 2.3002 - val_accuracy: 0.2222 - val_loss: 1.8209\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5s/step - accuracy: 0.1319 - loss: 2.0001 - val_accuracy: 0.2222 - val_loss: 1.7900\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 5s/step - accuracy: 0.1578 - loss: 1.8586 - val_accuracy: 0.2778 - val_loss: 1.7592\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.2459 - loss: 1.7687 - val_accuracy: 0.2222 - val_loss: 1.7634\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 5s/step - accuracy: 0.2170 - loss: 1.7705 - val_accuracy: 0.2222 - val_loss: 1.7677\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 5s/step - accuracy: 0.1790 - loss: 1.7909 - val_accuracy: 0.2222 - val_loss: 1.7628\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 5s/step - accuracy: 0.2303 - loss: 1.7450 - val_accuracy: 0.1667 - val_loss: 1.7789\n"
     ]
    }
   ],
   "source": [
    "# RESNET\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Load ResNet50 without the top layer\n",
    "resnet_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in resnet_base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = Flatten()(resnet_base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(animal_classes), activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "resnet_model = Model(inputs=resnet_base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "resnet_history = resnet_model.fit(train_data, epochs=10, validation_data=val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce4390-32a1-4f28-a4eb-5723142f504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 13s/step - accuracy: 0.2967 - loss: 2.1639 - val_accuracy: 0.2222 - val_loss: 1.9228\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 14s/step - accuracy: 0.2577 - loss: 1.8062 - val_accuracy: 0.4444 - val_loss: 1.6146\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 19s/step - accuracy: 0.3296 - loss: 1.6114 - val_accuracy: 0.3889 - val_loss: 1.4056\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 14s/step - accuracy: 0.4685 - loss: 1.2815 - val_accuracy: 0.5000 - val_loss: 1.2585\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21s/step - accuracy: 0.5743 - loss: 1.1093 - val_accuracy: 0.6111 - val_loss: 1.1603\n",
      "Epoch 6/10\n"
     ]
    }
   ],
   "source": [
    "# VGG19\n",
    "\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "# Load VGG19 without the top layer\n",
    "vgg_base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in vgg_base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers\n",
    "x = Flatten()(vgg_base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(animal_classes), activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "vgg_model = Model(inputs=vgg_base_model.input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "vgg_model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "vgg_history = vgg_model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7d302-b4a5-44d0-ad67-edb8452f536d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
